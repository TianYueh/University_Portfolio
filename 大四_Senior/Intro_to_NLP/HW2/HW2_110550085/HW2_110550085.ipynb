{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9733278,"sourceType":"datasetVersion","datasetId":5956674}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Use Synonym and Random Insertion/Deletion","metadata":{}},{"cell_type":"code","source":"import nltk\nimport os\n\n# 指定自定義下載目錄\nnltk_data_path = os.path.expanduser('~/nltk_data')\nif not os.path.exists(nltk_data_path):\n    os.makedirs(nltk_data_path)\n\n# 告訴 nltk 在指定目錄中查找資源\nnltk.data.path.append(nltk_data_path)\n\n# 下載所需資源\nnltk.download('wordnet', download_dir=nltk_data_path)\nnltk.download('omw-1.4', download_dir=nltk_data_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T06:38:42.003380Z","iopub.execute_input":"2024-11-19T06:38:42.003619Z","iopub.status.idle":"2024-11-19T06:38:43.946572Z","shell.execute_reply.started":"2024-11-19T06:38:42.003587Z","shell.execute_reply":"2024-11-19T06:38:43.945517Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import zipfile\nimport os\n\n# 解壓縮 wordnet 和 omw-1.4\nnltk_data_path = '/root/nltk_data/corpora/'\n\n# 解壓 wordnet.zip\nwordnet_zip = os.path.join(nltk_data_path, 'wordnet.zip')\nif os.path.exists(wordnet_zip):\n    with zipfile.ZipFile(wordnet_zip, 'r') as zip_ref:\n        zip_ref.extractall(nltk_data_path)\n\n# 解壓 omw-1.4.zip\nomw_zip = os.path.join(nltk_data_path, 'omw-1.4.zip')\nif os.path.exists(omw_zip):\n    with zipfile.ZipFile(omw_zip, 'r') as zip_ref:\n        zip_ref.extractall(nltk_data_path)\n\nprint(\"WordNet and OMw-1.4 have been extracted.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T06:38:43.947894Z","iopub.execute_input":"2024-11-19T06:38:43.948271Z","iopub.status.idle":"2024-11-19T06:38:44.715400Z","shell.execute_reply.started":"2024-11-19T06:38:43.948226Z","shell.execute_reply":"2024-11-19T06:38:44.714325Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"WordNet and OMw-1.4 have been extracted.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet\n\n# 測試 WordNet 是否能夠正常工作\ntry:\n    synonyms = wordnet.synsets(\"number\")\n    print(\"WordNet is working. Found synonyms for 'number':\", synonyms)\nexcept Exception as e:\n    print(\"Error loading WordNet:\", e)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T06:38:47.064970Z","iopub.execute_input":"2024-11-19T06:38:47.065751Z","iopub.status.idle":"2024-11-19T06:38:48.287295Z","shell.execute_reply.started":"2024-11-19T06:38:47.065716Z","shell.execute_reply":"2024-11-19T06:38:48.286429Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"WordNet is working. Found synonyms for 'number': [Synset('number.n.01'), Synset('number.n.02'), Synset('act.n.04'), Synset('phone_number.n.01'), Synset('numeral.n.01'), Synset('issue.n.02'), Synset('number.n.07'), Synset('number.n.08'), Synset('number.n.09'), Synset('number.n.10'), Synset('number.n.11'), Synset('total.v.01'), Synset('number.v.02'), Synset('number.v.03'), Synset('count.v.05'), Synset('count.v.01'), Synset('number.v.06')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save Only result.csv","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import f1_score\nimport numpy as np\nimport random\nimport nltk\nfrom nltk.corpus import wordnet\n\n# Disable Weights and Biases and logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Download the WordNet data if not already downloaded\nnltk.download('wordnet')\n\n# Load train, validation, and test data\nwith open('/kaggle/input/inlphw2dataset/train.json', 'r') as f:\n    train_data = json.load(f)\n\nwith open('/kaggle/input/inlphw2dataset/val.json', 'r') as f:\n    val_data = json.load(f)\n\nwith open('/kaggle/input/inlphw2dataset/test.json', 'r') as f:\n    test_data = json.load(f)\n\n# Load the sample submission format to get label columns\nsample_submission = pd.read_csv('/kaggle/input/inlphw2dataset/sample_submission.csv')\nlabel_columns = sample_submission.columns[1:]  # Label columns\n\n# Data Augmentation Functions\ndef synonym_replacement(text, n=1):\n    \"\"\"\n    Replaces `n` words in the text with their synonyms.\n    \"\"\"\n    words = text.split()\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n    random.shuffle(random_word_list)\n\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = wordnet.synsets(random_word)\n        if synonyms:\n            synonym = synonyms[0].lemmas()[0].name()  # Pick the first synonym\n            if synonym != random_word:\n                new_words = [synonym if word == random_word else word for word in new_words]\n                num_replaced += 1\n            if num_replaced >= n:\n                break\n\n    return ' '.join(new_words)\n\ndef random_insertion(text, n=1):\n    \"\"\"\n    Inserts `n` random synonyms of existing words into the text at random positions.\n    \"\"\"\n    words = text.split()\n    new_words = words.copy()\n    \n    for _ in range(n):\n        random_word = random.choice(words)\n        synonyms = wordnet.synsets(random_word)\n        if synonyms:\n            synonym = synonyms[0].lemmas()[0].name()\n            random_idx = random.randint(0, len(new_words))\n            new_words.insert(random_idx, synonym)\n    \n    return ' '.join(new_words)\n\ndef random_deletion(text, p=0.1):\n    \"\"\"\n    Randomly deletes words from the text with a probability `p`.\n    \"\"\"\n    words = text.split()\n    if len(words) == 1:\n        return text  # Avoid deleting the only word in text\n\n    new_words = [word for word in words if random.uniform(0, 1) > p]\n    if not new_words:\n        return random.choice(words)  # Avoid empty string\n\n    return ' '.join(new_words)\n\n# Prepare the training data with augmentation\ntrain_texts = [entry[\"tweet\"] for entry in train_data]\ntrain_labels = [{label: 1 for label in entry[\"labels\"].keys()} for entry in train_data]\n\n# Apply data augmentation\naugmented_train_texts = []\naugmented_train_labels = []\n\nfor text, labels in zip(train_texts, train_labels):\n    augmented_train_texts.append(text)  # Original text\n    augmented_train_labels.append(labels)\n    \n    # Synonym Replacement\n    augmented_train_texts.append(synonym_replacement(text, n=2))\n    augmented_train_labels.append(labels)\n    \n    # Random Insertion\n    augmented_train_texts.append(random_insertion(text, n=2))\n    augmented_train_labels.append(labels)\n    \n    # Random Deletion\n    augmented_train_texts.append(random_deletion(text, p=0.3))\n    augmented_train_labels.append(labels)\n\n# Prepare the validation data\nval_texts = [entry[\"tweet\"] for entry in val_data]\nval_labels = [{label: 1 for label in entry[\"labels\"].keys()} for entry in val_data]\n\n# Map labels to indices\nlabel_to_id = {label: idx for idx, label in enumerate(label_columns)}\nid_to_label = {idx: label for label, idx in label_to_id.items()}\n\n# Convert labels to a binary matrix format\ndef convert_to_multilabel(labels_list, label_to_id):\n    multilabel = np.zeros((len(labels_list), len(label_to_id)), dtype=int)\n    for i, labels in enumerate(labels_list):\n        for label in labels:\n            multilabel[i][label_to_id[label]] = 1\n    return multilabel\n\ntrain_labels = convert_to_multilabel(augmented_train_labels, label_to_id)\nval_labels = convert_to_multilabel(val_labels, label_to_id)\n\n# Prepare the test data\ntest_texts = [entry[\"tweet\"] for entry in test_data]\ntest_ids = [entry[\"ID\"] for entry in test_data]\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Define custom dataset class for BERT\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels=None):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n\n# Create train, validation, and test datasets\ntrain_dataset = TweetDataset(augmented_train_texts, train_labels)\nval_dataset = TweetDataset(val_texts, val_labels)\ntest_dataset = TweetDataset(test_texts)\n\n# Initialize the BERT model for multilabel classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_columns))\n\n# Define a custom compute_metrics function for strict macro F1 score calculation\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions >= 0.5  # Threshold for multilabel classification\n    macro_f1 = f1_score(labels, preds, average='macro')\n    return {\"macro_f1\": macro_f1}\n\n# Set training arguments with minimal logging\ntraining_args = TrainingArguments(\n    output_dir='./results',  # Required but will only store checkpoints here\n    eval_strategy=\"epoch\",  # Perform evaluation at the end of each epoch\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_dir=None,  # Disable logging directory to avoid saving log files\n    logging_steps=5000,  # Set high logging steps to avoid frequent logs\n    save_strategy=\"no\",  # Disable checkpoint saving\n)\n\n# Define the Trainer with validation and compute_metrics\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # Use val.json as validation dataset\n    compute_metrics=compute_metrics  # Calculate macro F1 score\n)\n\n# Train the model\ntrainer.train()\n\n# Make predictions on the test set\npredictions = trainer.predict(test_dataset)\npred_labels = (torch.sigmoid(torch.tensor(predictions.predictions)) >= 0.5).int().numpy()\n\n# Create submission file in the correct format\nsubmission = pd.DataFrame(pred_labels, columns=label_columns)\nsubmission.insert(0, \"index\", test_ids)  # Renaming the first column to \"index\"\nsubmission.to_csv('/kaggle/working/result.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T06:38:50.608943Z","iopub.execute_input":"2024-11-19T06:38:50.609562Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0861d565ad1b4eb8be8cb6bbcb438e5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d5120041b4446db73357c961361eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddea43e0f37940139ed23aca474c8cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757cd47453f6443d993cdaa6a4c0ead7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e8aa9d67ab842b7a0bf018f9c6395a6"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4679' max='13912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4679/13912 09:50 < 19:24, 7.93 it/s, Epoch 1.35/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.150629</td>\n      <td>0.537036</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]}]}