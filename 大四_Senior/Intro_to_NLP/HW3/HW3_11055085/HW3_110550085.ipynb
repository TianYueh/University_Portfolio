{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10199325,"sourceType":"datasetVersion","datasetId":6302391},{"sourceId":10248979,"sourceType":"datasetVersion","datasetId":6339086}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ver3","metadata":{}},{"cell_type":"code","source":"!pip install spellchecker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T11:30:56.769865Z","iopub.execute_input":"2024-12-19T11:30:56.770601Z","iopub.status.idle":"2024-12-19T11:31:21.085262Z","shell.execute_reply.started":"2024-12-19T11:30:56.770564Z","shell.execute_reply":"2024-12-19T11:31:21.084148Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting spellchecker\n  Downloading spellchecker-0.4.tar.gz (3.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spellchecker) (70.0.0)\nCollecting inexactsearch (from spellchecker)\n  Downloading inexactsearch-1.0.2.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting soundex>=1.0 (from inexactsearch->spellchecker)\n  Downloading soundex-1.1.3.tar.gz (9.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting silpa_common>=0.3 (from inexactsearch->spellchecker)\n  Downloading silpa_common-0.3.tar.gz (9.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: spellchecker, inexactsearch, silpa_common, soundex\n  Building wheel for spellchecker (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for spellchecker: filename=spellchecker-0.4-py3-none-any.whl size=3966500 sha256=577c237028c0ee6626c663315be5a0f410181da77feb1d5fbd1394067cdcb2bb\n  Stored in directory: /root/.cache/pip/wheels/6c/90/c3/eac248d8755b2a7343487a2087b4b29ad98f388c3c8c69c286\n  Building wheel for inexactsearch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for inexactsearch: filename=inexactsearch-1.0.2-py3-none-any.whl size=7123 sha256=c13759e3953dad712a18ba8fdc7167ff01f88e0e2ea457cbcd97b17bea82b53d\n  Stored in directory: /root/.cache/pip/wheels/63/19/2c/5e9f447f2533d457a1167c3e553f235e232b8a639e3f5fafab\n  Building wheel for silpa_common (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for silpa_common: filename=silpa_common-0.3-py3-none-any.whl size=8469 sha256=98199f0fca82cbc26f183f9f8ad5ffaebe4b4cf6d108e26a01544513db9150a1\n  Stored in directory: /root/.cache/pip/wheels/c0/72/43/0c779b79d708c78240beb3b0bb8f5ff3c2ab81c4e5271ea1aa\n  Building wheel for soundex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for soundex: filename=soundex-1.1.3-py3-none-any.whl size=8876 sha256=569105922460417f1250abb4998ffdd9ca374967bdb10007c5b03d1e334ba2f2\n  Stored in directory: /root/.cache/pip/wheels/a7/c7/c0/99e0278924f5664ab201bee9eee6e7a856caabf95a6fe008c5\nSuccessfully built spellchecker inexactsearch silpa_common soundex\nInstalling collected packages: silpa_common, soundex, inexactsearch, spellchecker\nSuccessfully installed inexactsearch-1.0.2 silpa_common-0.3 soundex-1.1.3 spellchecker-0.4\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install googletrans==4.0.0-rc1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T11:18:30.735735Z","iopub.execute_input":"2024-12-19T11:18:30.736587Z","iopub.status.idle":"2024-12-19T11:18:43.639163Z","shell.execute_reply.started":"2024-12-19T11:18:30.736540Z","shell.execute_reply":"2024-12-19T11:18:43.637983Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=54c3b9b77a43e90a89ca222c1e3ca6600b3950e1058f14d93ec35ec98eccf56f\n  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.7\n    Uninstalling idna-3.7:\n      Successfully uninstalled idna-3.7\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.5\n    Uninstalling httpcore-1.0.5:\n      Successfully uninstalled httpcore-1.0.5\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Uninstalling httpx-0.27.0:\n      Successfully uninstalled httpx-0.27.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.3.1 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import nltk\nimport os\n\n# 指定自定義下載目錄\nnltk_data_path = os.path.expanduser('~/nltk_data')\nif not os.path.exists(nltk_data_path):\n    os.makedirs(nltk_data_path)\n\n# 告訴 nltk 在指定目錄中查找資源\nnltk.data.path.append(nltk_data_path)\n\n# 下載所需資源\nnltk.download('wordnet', download_dir=nltk_data_path)\nnltk.download('omw-1.4', download_dir=nltk_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:28:19.011042Z","iopub.execute_input":"2024-12-19T10:28:19.011396Z","iopub.status.idle":"2024-12-19T10:28:19.274022Z","shell.execute_reply.started":"2024-12-19T10:28:19.011363Z","shell.execute_reply":"2024-12-19T10:28:19.273163Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import zipfile\nimport os\n\n# 解壓縮 wordnet 和 omw-1.4\nnltk_data_path = '/root/nltk_data/corpora/'\n\n# 解壓 wordnet.zip\nwordnet_zip = os.path.join(nltk_data_path, 'wordnet.zip')\nif os.path.exists(wordnet_zip):\n    with zipfile.ZipFile(wordnet_zip, 'r') as zip_ref:\n        zip_ref.extractall(nltk_data_path)\n\n# 解壓 omw-1.4.zip\nomw_zip = os.path.join(nltk_data_path, 'omw-1.4.zip')\nif os.path.exists(omw_zip):\n    with zipfile.ZipFile(omw_zip, 'r') as zip_ref:\n        zip_ref.extractall(nltk_data_path)\n\nprint(\"WordNet and OMw-1.4 have been extracted.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:28:21.020827Z","iopub.execute_input":"2024-12-19T10:28:21.021197Z","iopub.status.idle":"2024-12-19T10:28:21.783427Z","shell.execute_reply.started":"2024-12-19T10:28:21.021165Z","shell.execute_reply":"2024-12-19T10:28:21.782518Z"}},"outputs":[{"name":"stdout","text":"WordNet and OMw-1.4 have been extracted.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom torch.utils.data import Dataset\nimport torch\nimport random\nimport nltk\nfrom nltk.corpus import wordnet\n\n# Ensure NLTK resources are downloaded\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\n# Paths for Kaggle dataset\ntrain_path = \"/kaggle/input/inlphw3dataset/HW3_dataset/train.json\"\nval_path = \"/kaggle/input/inlphw3dataset/HW3_dataset/val.json\"\ntest_path = \"/kaggle/input/inlphw3dataset/HW3_dataset/test.json\"\nsubmission_path = \"/kaggle/working/sample_submission.csv\"\n\n# Load training, validation, and test data\nwith open(train_path, \"r\") as f:\n    train_data = json.load(f)\nwith open(val_path, \"r\") as f:\n    val_data = json.load(f)\nwith open(test_path, \"r\") as f:\n    test_data = json.load(f)\n\nprint(f\"Number of entries in train.json: {len(train_data)}\")\n\n# Check for data leakage\ntrain_set = set([f\"{item['u']} {' '.join(item['s'])} {item['r']}\" for item in train_data])\nval_set = set([f\"{item['u']} {' '.join(item['s'])} {item['r']}\" for item in val_data])\noverlap = train_set.intersection(val_set)\nif overlap:\n    print(f\"Data leakage detected: {len(overlap)} overlapping samples\")\n    print(\"Examples of overlapping data:\", list(overlap)[:5])\nelse:\n    print(\"No data leakage detected between training and validation datasets.\")\n\n# Synonym replacement function for data augmentation\ndef synonym_replacement(sentence, n=2):\n    words = sentence.split()\n    new_words = words.copy()\n    random.shuffle(words)\n    replaced = 0\n\n    for word in words:\n        synonyms = wordnet.synsets(word)\n        if synonyms:\n            synonym = synonyms[0].lemmas()[0].name()\n            if synonym != word:\n                new_words = [synonym if w == word else w for w in new_words]\n                replaced += 1\n        if replaced >= n:\n            break\n\n    return \" \".join(new_words)\n\n# Additional augmentation: random deletion\ndef random_deletion(sentence, p=0.1):\n    words = sentence.split()\n    if len(words) == 1:\n        return sentence\n\n    new_words = [word for word in words if random.uniform(0, 1) > p]\n    if len(new_words) == 0:\n        return words[random.randint(0, len(words) - 1)]\n\n    return \" \".join(new_words)\n\n# Augment data by applying synonym replacement and random deletion\ndef augment_data(data, augmentation_factor=1):\n    augmented_data = []\n    for item in data:\n        original_text = f\"Q: {item['u']} S: {' '.join(item['s'])} R: {item['r']}\"\n        augmented_data.append(item)\n        for _ in range(augmentation_factor):\n            augmented_text = synonym_replacement(original_text)\n            augmented_text = random_deletion(augmented_text)\n            \n            # Validate augmented_text structure\n            if 'S:' not in augmented_text or 'R:' not in augmented_text:\n                continue  # Skip this augmentation if structure is invalid\n            \n            augmented_item = item.copy()\n            try:\n                augmented_item['u'] = augmented_text.split('S:')[0].replace('Q: ', '').strip()\n                augmented_item['s'] = augmented_text.split('R:')[0].split('S:')[1].strip().split()\n                augmented_item['r'] = augmented_text.split('R:')[1].strip()\n                augmented_data.append(augmented_item)\n            except IndexError:\n                # Skip this augmented sample if any parsing issue occurs\n                continue\n    return augmented_data\n\n# Prepare data\ndef prepare_data(data, for_training=True):\n    texts = [f\"Q: {item['u']} S: {' '.join(item['s'])} R: {item['r']}\" for item in data]\n    labels = [item['r.label'] for item in data] if for_training else None\n    return texts, labels\n\n# Apply augmentation to the training dataset\naugmented_train_data = augment_data(train_data, augmentation_factor=2)\nX_train, y_train = prepare_data(augmented_train_data)\nX_val, y_val = prepare_data(val_data)\nX_test, _ = prepare_data(test_data, for_training=False)\n\n# Custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoded = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        item = {key: val.squeeze(0) for key, val in encoded.items()}  # Remove batch dimension\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n# Initialize tokenizer for a stronger model (RoBERTa large)\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n\n# Create datasets\ntrain_dataset = CustomDataset(X_train, y_train, tokenizer=tokenizer)\nval_dataset = CustomDataset(X_val, y_val, tokenizer=tokenizer)\ntest_dataset = CustomDataset(X_test, labels=None, tokenizer=tokenizer)\n\n# Load RoBERTa model for classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n\n# Compute metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=1).numpy()\n    precision = precision_score(labels, predictions, average=\"binary\")\n    recall = recall_score(labels, predictions, average=\"binary\")\n    f1 = f1_score(labels, predictions, average=\"binary\")\n    accuracy = accuracy_score(labels, predictions)\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Training arguments with mixed precision\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"/kaggle/working/logs\",\n    per_device_train_batch_size=8,  # Reduce batch size to fit larger models\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=4,  # Simulate a larger batch size\n    learning_rate=2e-5,  # Adjust learning rate\n    num_train_epochs=2,  # Increase epochs\n    warmup_steps=500,  # Larger warmup\n    weight_decay=0.01,\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    fp16=True  # Mixed precision for faster training\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Predict\npredictions = trainer.predict(test_dataset)\npredicted_labels = predictions.predictions.argmax(axis=1)\n\n# Save submission\nsubmission = pd.DataFrame({\n    \"index\": range(len(predicted_labels)),\n    \"response_quality\": predicted_labels\n})\nsubmission.to_csv(submission_path, index=False)\nprint(f\"Submission file saved to {submission_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T11:40:14.186156Z","iopub.execute_input":"2024-12-19T11:40:14.186480Z","iopub.status.idle":"2024-12-19T12:38:02.734977Z","shell.execute_reply.started":"2024-12-19T11:40:14.186450Z","shell.execute_reply":"2024-12-19T12:38:02.734091Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nNumber of entries in train.json: 3696\nNo data leakage detected between training and validation datasets.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='604' max='604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [604/604 56:59, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.665100</td>\n      <td>0.640463</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.465700</td>\n      <td>0.488656</td>\n      <td>0.768939</td>\n      <td>0.670886</td>\n      <td>0.602273</td>\n      <td>0.634731</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Submission file saved to /kaggle/working/sample_submission.csv\n","output_type":"stream"}],"execution_count":20}]}